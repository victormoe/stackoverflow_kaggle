{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29287731",
   "metadata": {},
   "source": [
    "# Load and discover data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e7d30a",
   "metadata": {},
   "source": [
    "## Partial load to play with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20650ef9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T21:40:35.757736Z",
     "start_time": "2021-10-11T21:40:29.455767Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell if you want to load data partially. Replace path by your data path.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "questions_train = pd.read_csv('/Users/victormoeneclaey/pickles/ml-challenge/questions_train.csv', nrows=10000)\n",
    "answers_train = pd.read_csv('/Users/victormoeneclaey/pickles/ml-challenge/answers_train.csv', nrows=10000)\n",
    "questions_test = pd.read_csv('/Users/victormoeneclaey/pickles/ml-challenge/questions_test.csv', nrows=10000)\n",
    "users = pd.read_csv('/Users/victormoeneclaey/pickles/ml-challenge/users.csv', nrows=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39d0ca2",
   "metadata": {},
   "source": [
    "## Full load to create submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4139657",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T21:44:03.760797Z",
     "start_time": "2021-10-11T21:42:38.096357Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell if you want to load full data. Replace path by your data path.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "questions_train = pd.read_csv('/Users/victormoeneclaey/pickles/ml-challenge/questions_train.csv')\n",
    "answers_train = pd.read_csv('/Users/victormoeneclaey/pickles/ml-challenge/answers_train.csv')\n",
    "questions_test = pd.read_csv('/Users/victormoeneclaey/pickles/ml-challenge/questions_test.csv')\n",
    "users = pd.read_csv('/Users/victormoeneclaey/pickles/ml-challenge/users.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43472f89",
   "metadata": {},
   "source": [
    "## Visualize data with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f210eebc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T21:44:04.287892Z",
     "start_time": "2021-10-11T21:44:03.814119Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>question_id</th>\n",
       "      <th>owner_user_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>61059891</td>\n",
       "      <td>12481897.0</td>\n",
       "      <td>folium.Marker does not does decode properly</td>\n",
       "      <td>&lt;p&gt;I have a folium map called imap.\\nI have cr...</td>\n",
       "      <td>2020-04-06T12:29:30.87</td>\n",
       "      <td>61060428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>61199055</td>\n",
       "      <td>13296781.0</td>\n",
       "      <td>Using QStackedWidget in PyQt5</td>\n",
       "      <td>&lt;p&gt;I have QStackedWidget in ApplicationWindow ...</td>\n",
       "      <td>2020-04-14T00:47:21.127</td>\n",
       "      <td>61200491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>61246717</td>\n",
       "      <td>12320080.0</td>\n",
       "      <td>Hadoop connection refused on port 9000. Virtua...</td>\n",
       "      <td>&lt;p&gt;I installed Hadoop on VirtualBox Ubuntu. Al...</td>\n",
       "      <td>2020-04-16T09:17:00.277</td>\n",
       "      <td>61255991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>61095077</td>\n",
       "      <td>7902677.0</td>\n",
       "      <td>pyscreenshot get remote windows server's scree...</td>\n",
       "      <td>&lt;p&gt;The python2 script was executed by the syst...</td>\n",
       "      <td>2020-04-08T07:07:34.197</td>\n",
       "      <td>61115533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>61179159</td>\n",
       "      <td>13297108.0</td>\n",
       "      <td>Can't construct a java object for tag:yaml.org...</td>\n",
       "      <td>&lt;p&gt;I'm trying to read yaml file contents using...</td>\n",
       "      <td>2020-04-12T22:41:11.913</td>\n",
       "      <td>61179673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  question_id  owner_user_id  \\\n",
       "0           0             1     61059891     12481897.0   \n",
       "1           1             2     61199055     13296781.0   \n",
       "2           2             3     61246717     12320080.0   \n",
       "3           3             4     61095077      7902677.0   \n",
       "4           4             5     61179159     13297108.0   \n",
       "\n",
       "                                               title  \\\n",
       "0        folium.Marker does not does decode properly   \n",
       "1                      Using QStackedWidget in PyQt5   \n",
       "2  Hadoop connection refused on port 9000. Virtua...   \n",
       "3  pyscreenshot get remote windows server's scree...   \n",
       "4  Can't construct a java object for tag:yaml.org...   \n",
       "\n",
       "                                                text                     date  \\\n",
       "0  <p>I have a folium map called imap.\\nI have cr...   2020-04-06T12:29:30.87   \n",
       "1  <p>I have QStackedWidget in ApplicationWindow ...  2020-04-14T00:47:21.127   \n",
       "2  <p>I installed Hadoop on VirtualBox Ubuntu. Al...  2020-04-16T09:17:00.277   \n",
       "3  <p>The python2 script was executed by the syst...  2020-04-08T07:07:34.197   \n",
       "4  <p>I'm trying to read yaml file contents using...  2020-04-12T22:41:11.913   \n",
       "\n",
       "   accepted_answer_id  \n",
       "0            61060428  \n",
       "1            61200491  \n",
       "2            61255991  \n",
       "3            61115533  \n",
       "4            61179673  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f48633f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T21:44:04.335214Z",
     "start_time": "2021-10-11T21:44:04.294030Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question_id</th>\n",
       "      <th>owner_user_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>60382284</td>\n",
       "      <td>6279751.0</td>\n",
       "      <td>Show/Hide Bokeh widgets based on another widget</td>\n",
       "      <td>&lt;p&gt;I have a checkbox A that has two options, a...</td>\n",
       "      <td>2020-02-24T18:54:16.74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>60471556</td>\n",
       "      <td>10045053.0</td>\n",
       "      <td>Why is composition of identity and unary funct...</td>\n",
       "      <td>&lt;p&gt;&lt;a href=\"https://mostly-adequate.gitbooks.i...</td>\n",
       "      <td>2020-03-01T02:45:29.663</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>60472988</td>\n",
       "      <td>11816815.0</td>\n",
       "      <td>How to create a shortcut key for a JLabel?</td>\n",
       "      <td>&lt;p&gt;&lt;a href=\"https://i.stack.imgur.com/74SmA.jp...</td>\n",
       "      <td>2020-03-01T07:39:21.583</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>60481026</td>\n",
       "      <td>12990519.0</td>\n",
       "      <td>text-align: center is not working properly?</td>\n",
       "      <td>&lt;p&gt;I used javascript to change the divs but af...</td>\n",
       "      <td>2020-03-02T00:13:59.287</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>60486309</td>\n",
       "      <td>11157436.0</td>\n",
       "      <td>Set flag to check if parameters are filled in ...</td>\n",
       "      <td>&lt;p&gt;I want to have a flag that checks if parame...</td>\n",
       "      <td>2020-03-02T09:54:59.683</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  question_id  owner_user_id  \\\n",
       "0           0     60382284      6279751.0   \n",
       "1           1     60471556     10045053.0   \n",
       "2           2     60472988     11816815.0   \n",
       "3           3     60481026     12990519.0   \n",
       "4           4     60486309     11157436.0   \n",
       "\n",
       "                                               title  \\\n",
       "0    Show/Hide Bokeh widgets based on another widget   \n",
       "1  Why is composition of identity and unary funct...   \n",
       "2         How to create a shortcut key for a JLabel?   \n",
       "3        text-align: center is not working properly?   \n",
       "4  Set flag to check if parameters are filled in ...   \n",
       "\n",
       "                                                text                     date  \\\n",
       "0  <p>I have a checkbox A that has two options, a...   2020-02-24T18:54:16.74   \n",
       "1  <p><a href=\"https://mostly-adequate.gitbooks.i...  2020-03-01T02:45:29.663   \n",
       "2  <p><a href=\"https://i.stack.imgur.com/74SmA.jp...  2020-03-01T07:39:21.583   \n",
       "3  <p>I used javascript to change the divs but af...  2020-03-02T00:13:59.287   \n",
       "4  <p>I want to have a flag that checks if parame...  2020-03-02T09:54:59.683   \n",
       "\n",
       "   accepted_answer_id  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36acef19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T21:44:04.561633Z",
     "start_time": "2021-10-11T21:44:04.344651Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>61781687</td>\n",
       "      <td>3925626.0</td>\n",
       "      <td>61781088</td>\n",
       "      <td>&lt;p&gt;Create two functions for checking authentic...</td>\n",
       "      <td>2020-05-13T18:01:49.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>64943917</td>\n",
       "      <td>3141792.0</td>\n",
       "      <td>64942024</td>\n",
       "      <td>&lt;p&gt;The API of &lt;code&gt;HttpClient&lt;/code&gt; doesn't ...</td>\n",
       "      <td>2020-11-21T14:04:13.203</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>60636986</td>\n",
       "      <td>7137090.0</td>\n",
       "      <td>60635829</td>\n",
       "      <td>&lt;p&gt;I would do the following manner.&lt;/p&gt;\\n\\n&lt;p&gt;...</td>\n",
       "      <td>2020-03-11T13:17:22.873</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>60644264</td>\n",
       "      <td>11874554.0</td>\n",
       "      <td>60644132</td>\n",
       "      <td>&lt;p&gt;I think it's rather simple if I get it righ...</td>\n",
       "      <td>2020-03-11T20:49:54.243</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>60647861</td>\n",
       "      <td>11602935.0</td>\n",
       "      <td>60637475</td>\n",
       "      <td>&lt;p&gt;You can't directly use dictionary in xaml t...</td>\n",
       "      <td>2020-03-12T04:59:05.37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  answer_id     user_id  question_id  \\\n",
       "0           0   61781687   3925626.0     61781088   \n",
       "1           1   64943917   3141792.0     64942024   \n",
       "2           2   60636986   7137090.0     60635829   \n",
       "3           3   60644264  11874554.0     60644132   \n",
       "4           4   60647861  11602935.0     60637475   \n",
       "\n",
       "                                                text                     date  \\\n",
       "0  <p>Create two functions for checking authentic...   2020-05-13T18:01:49.93   \n",
       "1  <p>The API of <code>HttpClient</code> doesn't ...  2020-11-21T14:04:13.203   \n",
       "2  <p>I would do the following manner.</p>\\n\\n<p>...  2020-03-11T13:17:22.873   \n",
       "3  <p>I think it's rather simple if I get it righ...  2020-03-11T20:49:54.243   \n",
       "4  <p>You can't directly use dictionary in xaml t...   2020-03-12T04:59:05.37   \n",
       "\n",
       "   score  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c71cad72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T21:44:04.639009Z",
     "start_time": "2021-10-11T21:44:04.580686Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>display_name</th>\n",
       "      <th>about_me</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>last_access_date</th>\n",
       "      <th>location</th>\n",
       "      <th>reputation</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>views</th>\n",
       "      <th>website_url</th>\n",
       "      <th>profile_image_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>568</td>\n",
       "      <td>David McGraw</td>\n",
       "      <td>&lt;p&gt;Grinding my way to the moon. Startups, prod...</td>\n",
       "      <td>2008-08-06 19:50:58.627 UTC</td>\n",
       "      <td>2020-10-29 18:57:14.293 UTC</td>\n",
       "      <td>Wichita, KS, United States</td>\n",
       "      <td>5019</td>\n",
       "      <td>207</td>\n",
       "      <td>37</td>\n",
       "      <td>614</td>\n",
       "      <td>https://www.moonlitsolutions.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1483</td>\n",
       "      <td>Grzegorz Gierlik</td>\n",
       "      <td>&lt;p&gt;Software developer, former team leader. \\nF...</td>\n",
       "      <td>2008-08-15 20:40:17.893 UTC</td>\n",
       "      <td>2020-12-03 12:38:44.23 UTC</td>\n",
       "      <td>Szczecin, Poland</td>\n",
       "      <td>10402</td>\n",
       "      <td>1277</td>\n",
       "      <td>4</td>\n",
       "      <td>1336</td>\n",
       "      <td>http://ggierlik.wordpress.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1544</td>\n",
       "      <td>jeffm</td>\n",
       "      <td>&lt;p&gt;C++, C#, Windows programmer&lt;/p&gt;</td>\n",
       "      <td>2008-08-16 14:15:09.933 UTC</td>\n",
       "      <td>2020-12-01 19:42:49.983 UTC</td>\n",
       "      <td>United States</td>\n",
       "      <td>2950</td>\n",
       "      <td>798</td>\n",
       "      <td>42</td>\n",
       "      <td>337</td>\n",
       "      <td>http://www.pearlsoftware.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2440</td>\n",
       "      <td>Sire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008-08-22 10:47:46.27 UTC</td>\n",
       "      <td>2020-12-03 18:54:37.033 UTC</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>3511</td>\n",
       "      <td>964</td>\n",
       "      <td>34</td>\n",
       "      <td>358</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://i.stack.imgur.com/aav7m.png?s=128&amp;g=1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3054</td>\n",
       "      <td>John Spurlock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008-08-26 13:48:36.693 UTC</td>\n",
       "      <td>2020-12-05 22:56:09.93 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1713</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>283</td>\n",
       "      <td>https://johnspurlock.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    id      display_name  \\\n",
       "0           0   568      David McGraw   \n",
       "1           1  1483  Grzegorz Gierlik   \n",
       "2           2  1544             jeffm   \n",
       "3           3  2440              Sire   \n",
       "4           4  3054     John Spurlock   \n",
       "\n",
       "                                            about_me  \\\n",
       "0  <p>Grinding my way to the moon. Startups, prod...   \n",
       "1  <p>Software developer, former team leader. \\nF...   \n",
       "2                 <p>C++, C#, Windows programmer</p>   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                 creation_date             last_access_date  \\\n",
       "0  2008-08-06 19:50:58.627 UTC  2020-10-29 18:57:14.293 UTC   \n",
       "1  2008-08-15 20:40:17.893 UTC   2020-12-03 12:38:44.23 UTC   \n",
       "2  2008-08-16 14:15:09.933 UTC  2020-12-01 19:42:49.983 UTC   \n",
       "3   2008-08-22 10:47:46.27 UTC  2020-12-03 18:54:37.033 UTC   \n",
       "4  2008-08-26 13:48:36.693 UTC   2020-12-05 22:56:09.93 UTC   \n",
       "\n",
       "                     location  reputation  up_votes  down_votes  views  \\\n",
       "0  Wichita, KS, United States        5019       207          37    614   \n",
       "1            Szczecin, Poland       10402      1277           4   1336   \n",
       "2               United States        2950       798          42    337   \n",
       "3                      Sweden        3511       964          34    358   \n",
       "4                         NaN        1713         2           0    283   \n",
       "\n",
       "                        website_url  \\\n",
       "0  https://www.moonlitsolutions.com   \n",
       "1     http://ggierlik.wordpress.com   \n",
       "2      http://www.pearlsoftware.com   \n",
       "3                               NaN   \n",
       "4          https://johnspurlock.com   \n",
       "\n",
       "                               profile_image_url  \n",
       "0                                            NaN  \n",
       "1                                            NaN  \n",
       "2                                            NaN  \n",
       "3  https://i.stack.imgur.com/aav7m.png?s=128&g=1  \n",
       "4                                            NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a13897b",
   "metadata": {},
   "source": [
    "## Load as arrays for computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1541aeec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T21:45:28.683890Z",
     "start_time": "2021-10-11T21:44:38.840478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions in train set: 482735\n",
      "Number of answers in train set: 686936\n",
      "Number of questions in test set: 120742\n",
      "Number of users: 4842850\n"
     ]
    }
   ],
   "source": [
    "# Load as np arrays\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "questions_train_array = questions_train.to_numpy()\n",
    "answers_train_array = answers_train.to_numpy()\n",
    "questions_test_array = questions_test.to_numpy()\n",
    "users_array = users.to_numpy()\n",
    "\n",
    "print(f\"Number of questions in train set: {len(questions_train)}\")\n",
    "print(f\"Number of answers in train set: {len(answers_train_array)}\")\n",
    "print(f\"Number of questions in test set: {len(questions_test_array)}\")\n",
    "print(f\"Number of users: {len(users_array)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93c40b4",
   "metadata": {},
   "source": [
    "# Ideas of features and algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8618df76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T23:17:11.777478Z",
     "start_time": "2021-10-11T23:17:11.615946Z"
    }
   },
   "source": [
    "Some interesting features might be:\n",
    "\n",
    "- Date:\n",
    "    - one can not answer to a question if he has not subscribed yet or connected too long ago\n",
    "    - answer more probable if the user's last connection is close to the answer date ?\n",
    "- Textual data:\n",
    "    - question titles\n",
    "    - question contents\n",
    "    - answer contents\n",
    "    - \"About me\" user sections, as well as the content of questions a given user has answered or asked to know the user better\n",
    "- Number of questions a user has answere in the text: the more one answers, the more one answers ?\n",
    "- User activity indicators (take logarithm):\n",
    "    - Reputation score\n",
    "    - Upvotes\n",
    "    - Downvotes\n",
    "- The shape of the questions.users graph itself\n",
    "\n",
    "<b>My idea:</b></br>\n",
    "To solve the cold start problem in recommendation on new test data, we perform tf-idf-based (alternatively transformers or any other NLP method-based)similarity scores between test questions and train questions</br>\n",
    "For each close train question, we compute scores for all users. Either use simple binary scores (1 if the user has answered the question, 0 else) or make it more regular/continuous with collaborative filtering (here we use ALS, but we could also use deep-learning based collaborative filtering etc).\n",
    "\n",
    "For each question in test, compute weighted means of scores of those users from close train questions, weighted by the\n",
    "tf-idf-based cosine similarity scores and keep the 20 users with the highest scores.\n",
    "\n",
    "This model can be further improved by combining similarity scores betwen the test question and the train question associated to a given user, with the ALS score, along with other features like log(1 + number of questions answered by user in train), log(1 + reputation score), log(1 + upvote score), tf-idf or transformers-based similarity between the test question (using title and content) and the user (using \"About me\" section on user or the content of questions the user has responded to or asked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2527b4c3",
   "metadata": {},
   "source": [
    "# Load useful maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc4f972",
   "metadata": {},
   "source": [
    "Here we load maps that will be useful in our further computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04b99333",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T21:58:25.602425Z",
     "start_time": "2021-10-11T21:54:30.081552Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7cf02c417ed4142b97e49a2fde83c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d567bce39749e2b20fe806d6c75e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d230a7dce7140588ccd44001172dc9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76dca84b9e874346b125468a5f389388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of answers per question is 25\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "# Easy access to question id or index (i.e. position in the array)\n",
    "question_id_to_question_index_train = {}\n",
    "question_index_to_question_id_train = {}\n",
    "# Link questions with their owners\n",
    "asked_question_id_to_user_id = {}\n",
    "user_id_to_asked_question_ids = defaultdict(list)\n",
    "# Get timestamp of each question\n",
    "asked_question_timestamps_train = []\n",
    "\n",
    "# Same on test\n",
    "question_id_to_question_index_test = {}\n",
    "question_index_to_question_id_test = {}\n",
    "asked_question_timestamps_test = []\n",
    "\n",
    "# Easy access to answer id or index\n",
    "answer_id_to_answer_index = {}\n",
    "answer_index_to_answer_id = {}\n",
    "# Easily find answers to a given question\n",
    "question_id_to_answer_indexes = defaultdict(list)\n",
    "answer_index_to_question_id = {}\n",
    "# Link answers with their authors\n",
    "answered_question_id_to_user_ids = defaultdict(list)\n",
    "user_id_to_answered_question_ids = defaultdict(list)\n",
    "\n",
    "# Easy access to user id or index\n",
    "user_id_to_idx = {}\n",
    "user_idx_to_id = {}\n",
    "# Useful user timestamps (inscription and last connextion (+ 1 day buffer))\n",
    "inscription_timestamps = []\n",
    "last_connection_timestamps_plus_one_day = []\n",
    "\n",
    "for i, question in tqdm(enumerate(questions_train_array)):\n",
    "    question_id = int(question[2])\n",
    "    user_id = np.nan if np.isnan(question[3]) else int(question[3])\n",
    "    question_title = question[4]\n",
    "    question_text = question[5]\n",
    "    question_date = question[6]\n",
    "    question_id_to_question_index_train[question_id] = i\n",
    "    question_index_to_question_id_train[i] = question_id\n",
    "    asked_question_id_to_user_id[question_id] = user_id\n",
    "    if not np.isnan(user_id):\n",
    "        user_id_to_asked_question_ids[user_id].append(question_id)\n",
    "    try:\n",
    "        asked_question_timestamps_train.append(datetime.strptime(question_date, '%Y-%m-%dT%H:%M:%S.%f'))\n",
    "    except ValueError:\n",
    "        asked_question_timestamps_train.append(datetime.strptime(question_date, '%Y-%m-%dT%H:%M:%S'))\n",
    "\n",
    "for i, question in tqdm(enumerate(questions_test_array)):\n",
    "    question_id = int(question[1])\n",
    "    user_id = np.nan if np.isnan(question[2]) else int(question[2])\n",
    "    question_title = question[3]\n",
    "    question_text = question[4]\n",
    "    question_date = question[5]\n",
    "    question_id_to_question_index_test[question_id] = i\n",
    "    question_index_to_question_id_test[i] = question_id\n",
    "    asked_question_id_to_user_id[question_id] = user_id\n",
    "    try:\n",
    "        asked_question_timestamps_test.append(datetime.strptime(question_date, '%Y-%m-%dT%H:%M:%S.%f'))\n",
    "    except ValueError:\n",
    "        asked_question_timestamps_test.append(datetime.strptime(question_date, '%Y-%m-%dT%H:%M:%S'))\n",
    "\n",
    "for i, answer in tqdm(enumerate(answers_train_array)):\n",
    "    answer_id = int(answer[1])\n",
    "    question_id = int(answer[3])\n",
    "    user_id = np.nan if np.isnan(answer[2]) else int(answer[2])\n",
    "    answer_id_to_answer_index[answer_id] = i\n",
    "    answer_index_to_answer_id[i] = answer_id\n",
    "    question_id_to_answer_indexes[question_id].append(i)\n",
    "    answer_index_to_question_id[i] = question_id\n",
    "    if not np.isnan(user_id):\n",
    "        answered_question_id_to_user_ids[question_id].append(user_id)\n",
    "        user_id_to_answered_question_ids[user_id].append(question_id)\n",
    "\n",
    "for i, user in tqdm(enumerate(users_array)):\n",
    "    user_id = int(user[1])\n",
    "    user_id_to_idx[user_id] = i\n",
    "    user_idx_to_id[i] = user_id\n",
    "    inscription_date = user[4]\n",
    "    last_connection_date = user[5]\n",
    "    try:\n",
    "        inscription_timestamps.append(datetime.strptime(inscription_date, '%Y-%m-%d %H:%M:%S.%f UTC'))\n",
    "    except ValueError:\n",
    "        inscription_timestamps.append(datetime.strptime(inscription_date, '%Y-%m-%d %H:%M:%S UTC'))\n",
    "    try:\n",
    "        last_connection_timestamps_plus_one_day.append(\n",
    "            datetime.strptime(last_connection_date, '%Y-%m-%d %H:%M:%S.%f UTC') + timedelta(days=1)\n",
    "        )\n",
    "    except ValueError:\n",
    "        last_connection_timestamps_plus_one_day.append(\n",
    "            datetime.strptime(last_connection_date, '%Y-%m-%d %H:%M:%S UTC') + timedelta(days=1)\n",
    "        )\n",
    "print(\n",
    "    f\"Maximum number of answers per question is {max([len(val) for val in question_id_to_answer_indexes.values()])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fa33917",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T21:58:26.665420Z",
     "start_time": "2021-10-11T21:58:25.619340Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5315e4bd98f547c6ad177d3557157113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test no test question is a train question\n",
    "\n",
    "for i, question in tqdm(enumerate(questions_test_array.tolist())):\n",
    "    question_id = int(question[1])\n",
    "    user_id = question[3]\n",
    "    if question_id in question_id_to_question_index_train:\n",
    "        print(question_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50c1a6f",
   "metadata": {},
   "source": [
    "# Preprocess textual data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b3ddd",
   "metadata": {},
   "source": [
    "To preprocess our data, we need to remove code tags, tokenize, remove stop words and numbers and finally \n",
    "stem/lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d08878a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T21:59:42.917861Z",
     "start_time": "2021-10-11T21:59:27.412683Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/victormoeneclaey/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Error loading RegexpTokenizer: Package 'RegexpTokenizer'\n",
      "[nltk_data]     not found in index\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/victormoeneclaey/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Error loading PorterStemmer: Package 'PorterStemmer' not\n",
      "[nltk_data]     found in index\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "#nltk.download(\"word_tokenize\")\n",
    "nltk.download(\"RegexpTokenizer\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"PorterStemmer\")\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "TOKENIZER = RegexpTokenizer(r'\\w+')\n",
    "STEMMER = PorterStemmer()\n",
    "\n",
    "def format_string(string):\n",
    "    try:\n",
    "        if np.isnan(string):\n",
    "            return \"\"\n",
    "    except:\n",
    "        soup = BeautifulSoup(string or \"\")\n",
    "        stemmed_text = STEMMER.stem(soup.get_text())\n",
    "        tokenized_text = TOKENIZER.tokenize(stemmed_text)\n",
    "        tokenized_text_without_stop_words = [\n",
    "            word\n",
    "            for word in tokenized_text\n",
    "            if word not in STOPWORDS\n",
    "            and not word.isdigit()\n",
    "        ]\n",
    "        return \" \".join(tokenized_text_without_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe80f4dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T22:38:15.619270Z",
     "start_time": "2021-10-11T22:01:24.603581Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a4c809a21045b29e1b10dbefed22cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/482735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc767b207ff4333af19ab9335c3061f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120742 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13f3434145e481783c42a57e3ea165d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/686936 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1082884536164fb6af2f0d8515777552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4842850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victormoeneclaey/py_38_env/lib/python3.8/site-packages/bs4/__init__.py:337: MarkupResemblesLocatorWarning: \".\" looks like a directory name, not markup. You may want to open a file found in this directory and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/victormoeneclaey/py_38_env/lib/python3.8/site-packages/bs4/__init__.py:431: MarkupResemblesLocatorWarning: \"http://forum.vi-vn.com\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/victormoeneclaey/py_38_env/lib/python3.8/site-packages/bs4/__init__.py:431: MarkupResemblesLocatorWarning: \"http://www.linkedin.com/in/raffaellobrondi\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/victormoeneclaey/py_38_env/lib/python3.8/site-packages/bs4/__init__.py:431: MarkupResemblesLocatorWarning: \"http://www.just-great-software.com/aboutjg.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/victormoeneclaey/py_38_env/lib/python3.8/site-packages/bs4/__init__.py:431: MarkupResemblesLocatorWarning: \"http://www.linkedin.com/in/petermaas\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/victormoeneclaey/py_38_env/lib/python3.8/site-packages/bs4/__init__.py:431: MarkupResemblesLocatorWarning: \"http://www.briandilley.com\n",
      "http://www.beeftank.com\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/victormoeneclaey/py_38_env/lib/python3.8/site-packages/bs4/__init__.py:431: MarkupResemblesLocatorWarning: \"http://www.google.com/profiles/hemanth.hm\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/victormoeneclaey/py_38_env/lib/python3.8/site-packages/bs4/__init__.py:431: MarkupResemblesLocatorWarning: \"http://stackoverflow.com/users/151955/karl-easterly\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/victormoeneclaey/py_38_env/lib/python3.8/site-packages/bs4/__init__.py:431: MarkupResemblesLocatorWarning: \"http://www.linkedin.com/in/joshuaewer\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/victormoeneclaey/py_38_env/lib/python3.8/site-packages/bs4/__init__.py:431: MarkupResemblesLocatorWarning: \"http://developingux.com/about/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/victormoeneclaey/py_38_env/lib/python3.8/site-packages/bs4/__init__.py:337: MarkupResemblesLocatorWarning: \"..\" looks like a directory name, not markup. You may want to open a file found in this directory and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/victormoeneclaey/py_38_env/lib/python3.8/site-packages/bs4/__init__.py:431: MarkupResemblesLocatorWarning: \"http://pl.linkedin.com/in/marcinwojciechowski\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/victormoeneclaey/py_38_env/lib/python3.8/site-packages/bs4/__init__.py:431: MarkupResemblesLocatorWarning: \"http://es.linkedin.com/in/fjfnaranjo\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/victormoeneclaey/py_38_env/lib/python3.8/site-packages/bs4/__init__.py:431: MarkupResemblesLocatorWarning: \"http://andersonf.posterous.com\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/victormoeneclaey/py_38_env/lib/python3.8/site-packages/bs4/__init__.py:431: MarkupResemblesLocatorWarning: \"http://twitter.com/ArtemR\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/victormoeneclaey/py_38_env/lib/python3.8/site-packages/bs4/__init__.py:431: MarkupResemblesLocatorWarning: \"http://vivekjuneja.blogspot.com\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/victormoeneclaey/py_38_env/lib/python3.8/site-packages/bs4/__init__.py:431: MarkupResemblesLocatorWarning: \"http://mathzen.wordpress.com\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/victormoeneclaey/py_38_env/lib/python3.8/site-packages/bs4/__init__.py:431: MarkupResemblesLocatorWarning: \"http://www.last.fm/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/victormoeneclaey/py_38_env/lib/python3.8/site-packages/bs4/__init__.py:431: MarkupResemblesLocatorWarning: \"http://hansvd.wordpress.com\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/victormoeneclaey/py_38_env/lib/python3.8/site-packages/bs4/__init__.py:431: MarkupResemblesLocatorWarning: \"http://cnev.ru/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/victormoeneclaey/py_38_env/lib/python3.8/site-packages/bs4/__init__.py:431: MarkupResemblesLocatorWarning: \"http://www.masonmc.com\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing train questions' title and content...\")\n",
    "for question in tqdm(questions_train_array):\n",
    "    if question[4]:\n",
    "        question[4] = format_string(question[4])\n",
    "    if question[5]:\n",
    "        question[5] = format_string(question[5])\n",
    "\n",
    "print(\"Preprocess test questions' title and content...\")\n",
    "for question in tqdm(questions_test_array):\n",
    "    if question[3]:\n",
    "        question[3] = format_string(question[3])\n",
    "    if question[4]:\n",
    "        question[4] = format_string(question[4])\n",
    "\n",
    "print(\"Preprocessing answers' content...\")\n",
    "# Not used by our solution but could be useful, for instance to get more information on the responding users\n",
    "for answer in tqdm(answers_train_array):\n",
    "    if answer[4]:\n",
    "        answer[4] = format_string(answer[4])\n",
    "\n",
    "print(\"Preprocessing users' About me section...\")\n",
    "# Not used by our solution but suggested use as an improvement\n",
    "for user in tqdm(users_array):\n",
    "    if user[3]:\n",
    "        user[3] = format_string(user[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d668c4",
   "metadata": {},
   "source": [
    "# Train tf-idf model and get similarities between questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15ce807",
   "metadata": {},
   "source": [
    "## Define and train tf-idf model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50fdcb4",
   "metadata": {},
   "source": [
    "To predict users for new test questions, the biggest clue we have is the content of the actual question. We here face a cold start problem in recommendation, and need to carry out content-base filtering. We can further enhance this we collaborative filtering to know the train data better.\n",
    "\n",
    "We train a tf-idf model on words that appear more than 100 times in train and test. The idea here is to get relevant words, both general and quite specific (stackoverflow is a technical website so rare words might be important to gather similar questions).\n",
    "\n",
    "But we still need to get a tractable embedding size for computing time purposes. The idea is thus to set a boundary on word scarcity, and couple this with a dimensionality reduction algorithm to make cosine similarity and knn computations feasible. This was quite a challenge.\n",
    "Hopefully, combining sparse matrices and the hnswlib iibrary solves this problem quite well and much faster than scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b965aa18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T22:42:30.332901Z",
     "start_time": "2021-10-11T22:38:15.640967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting tf-idf...\n",
      "Vocabulary size is 18356\n",
      "Computing embeddings...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(min_df = 100)\n",
    "\n",
    "print(\"Fitting tf-idf...\")\n",
    "\n",
    "tf_idf_vectorizer.fit(\n",
    "    [\n",
    "        elt for question in questions_train_array for elt in [question[4], question[5]]\n",
    "    ] + [\n",
    "        elt for question in questions_test_array for elt in [question[3], question[4]]\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Vocabulary size is {len(tf_idf_vectorizer.vocabulary_)}\")\n",
    "\n",
    "print(\"Computing embeddings...\")\n",
    "train_question_title_embeddings = tf_idf_vectorizer.transform([question[4] for question in questions_train_array])\n",
    "train_question_text_embeddings = tf_idf_vectorizer.transform([question[4] for question in questions_train_array])\n",
    "test_question_title_embeddings = tf_idf_vectorizer.transform([question[3] for question in questions_test_array])\n",
    "test_question_text_embeddings = tf_idf_vectorizer.transform([question[4] for question in questions_test_array])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ff8c5",
   "metadata": {},
   "source": [
    "## Alternatively, we could have used NLP transformers model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b829f36",
   "metadata": {},
   "source": [
    "We could also use state-of-the-art NLP pretrained embeddings, using models like BERT\n",
    "\n",
    "Potential advantages:\n",
    "- pre-trained sentence embedding model\n",
    "- large vocabulary\n",
    "- direct embedding in a tractable dimension\n",
    "- efficient understanding of natural language\n",
    "\n",
    "Potential disadvantages:\n",
    "- might not know (or know well) some specific technical words that may appear on stackoverflow\n",
    "- slower to predict than tf-idf\n",
    "- more like a black box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4db895d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T22:42:41.789524Z",
     "start_time": "2021-10-11T22:42:30.339620Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "transformers_vectorizer = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807b516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasts 1h-1h30\n",
    "\n",
    "print(\"Computing transformers embeddings...\")\n",
    "train_question_title_transformers_embeddings = transformers_vectorizer.encode([question[4] for question in questions_train_array])\n",
    "train_question_text_transformers_embeddings = transformers_vectorizer.encode([question[4] for question in questions_train_array])\n",
    "test_question_title_transformers_embeddings = transformers_vectorizer.encode([question[3] for question in questions_test_array])\n",
    "test_question_text_transformers_embeddings = transformers_vectorizer.encode([question[4] for question in questions_test_array])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d704cfc9",
   "metadata": {},
   "source": [
    "## Dimensionality reduction in dimension 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45672ab6",
   "metadata": {},
   "source": [
    "The vocabulary size remains large, and we need to carry out dimensionality reduction. Scikit PCA does not work on sparse matrix, we used SVD instead.\n",
    "We make the decision to compute question embedding by summing representations of the title and of the content, while giving more imoortance to the title (weight 2 vs 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aece60cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T22:48:00.611131Z",
     "start_time": "2021-10-11T22:48:00.611105Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy import sparse\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "truncated_svd = TruncatedSVD(100)\n",
    "\n",
    "print(\"Defining questions embeddings...\")\"\n",
    "train_question_sparse_embeddings = 2*sparse.csr_matrix(\n",
    "    train_question_title_embeddings\n",
    ") + sparse.csr_matrix(train_question_text_embeddings)\n",
    "\n",
    "test_question_sparse_embeddings = 2*sparse.csr_matrix(\n",
    "    test_question_title_embeddings\n",
    ") + sparse.csr_matrix(test_question_text_embeddings)\n",
    "question_sparse_embeddings = vstack((train_question_sparse_embeddings, test_question_sparse_embeddings))\n",
    "\n",
    "print(\"Fitting the model and transforming the embeddings...\")\n",
    "small_embeddings = truncated_svd.fit_transform(question_sparse_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df923e68",
   "metadata": {},
   "source": [
    "## Compute the similarities between questions and keep 200 closest questions for each question in train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e9f215",
   "metadata": {},
   "source": [
    "The hnswlib library is an approached solution and is much faster than scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "874aa5a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T22:47:59.078298Z",
     "start_time": "2021-10-11T22:46:13.015408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining a k-nearest-neighbours index with cosine similarity in dimension 100...\n",
      "Initializing the index...\n",
      "Inserting 482735 train questions in the index...\n"
     ]
    }
   ],
   "source": [
    "import hnswlib\n",
    "\n",
    "print(\"Defining a k-nearest-neighbours index with cosine similarity in dimension 100...\")\n",
    "index_cosine = hnswlib.Index(space = 'cosine', dim = 100)\n",
    "\n",
    "print(\"Initializing the index...\")\n",
    "index_cosine.init_index(max_elements = len(questions_train_array), ef_construction = 300, M = 16)\n",
    "\n",
    "print(f\"Inserting {len(questions_train_array)} train questions in the index...\")\n",
    "index_cosine.add_items(small_embeddings[:len(questions_train_array)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee7aa8f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T22:49:45.332398Z",
     "start_time": "2021-10-11T22:48:17.938809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute closest train question indexes as well as the respective cosine similarirites...\n"
     ]
    }
   ],
   "source": [
    "print(\"Compute closest train question indexes as well as the respective cosine similarirites...\")\n",
    "labels, distances = index_cosine.knn_query(small_embeddings, k = 200)\n",
    "question_similarities = 1 - distances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5d031aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T22:52:22.649198Z",
     "start_time": "2021-10-11T22:52:19.670994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of questions (train + test) mapped to other train questions with cosine >= 0.3 is 0.9995161373175779\n",
      "The proportion of questions (train + test) mapped to other train questions with cosine >= 0.5 is 0.9982037426447072\n"
     ]
    }
   ],
   "source": [
    "# Almost all questions are mapped to 200 very similar other questions \n",
    "\n",
    "proportion_1 = len(\n",
    "    [\n",
    "        i\n",
    "        for i in range(len(question_similarities))\n",
    "        if question_similarities[i][-1] >= 0.3\n",
    "    ]\n",
    ")/len(question_similarities)\n",
    "\n",
    "print(\n",
    "    f\"The proportion of questions (train + test) mapped to other train questions with cosine >= 0.3 is {proportion_1 }\"\n",
    ")\n",
    "\n",
    "proportion_2 = len(\n",
    "    [\n",
    "        i\n",
    "        for i in range(len(question_similarities))\n",
    "        if question_similarities[i][-1] >= 0.5\n",
    "    ]\n",
    ")/len(question_similarities)\n",
    "    \n",
    "print(\n",
    "    f\"The proportion of questions (train + test) mapped to other train questions with cosine >= 0.5 is {proportion_2}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f586e9d",
   "metadata": {},
   "source": [
    "# \"Simple\" model: for each test question, find closest train questions and get their users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcc7bdb",
   "metadata": {},
   "source": [
    "This model is our first model and is referred to as the \"simple\" model. We will see that this model is actually quite powerful compared to other more \"elaborate\" models. In real life, few respondents seem to be very active and answer most same theme-related questions.\n",
    "\n",
    "For each test question, we take our 200 closest questions and get their responding users. They are very few but actually seem to be very relevant according to the metrics. We simply associate those users wirh a score that is the similarity between the questions, summing up if the same user appears several times in different train questions. We then only keep the 20 \"best\" users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c6c551",
   "metadata": {},
   "source": [
    "## Metrics definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82a6da59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T22:52:37.889244Z",
     "start_time": "2021-10-11T22:52:37.875280Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_metrics_on_ordered_user_ids(ordered_user_ids, true_user_ids, all_user_ids):\n",
    "    mean_reciprocal_rank = 1/(1+len(all_user_ids))\n",
    "    for i, user_id in enumerate(ordered_user_ids):\n",
    "        if user_id in true_user_ids:\n",
    "            mean_reciprocal_rank = 1/(1+i)\n",
    "            break\n",
    "            \n",
    "    return {\n",
    "        \"precision@1\": np.sum([user_id in true_user_ids for user_id in ordered_user_ids[:1]]),\n",
    "        \"precision@5\": np.sum([user_id in true_user_ids for user_id in ordered_user_ids[:5]])/5,\n",
    "        \"precision@20\": np.sum([user_id in true_user_ids for user_id in ordered_user_ids[:20]])/20,\n",
    "        \"mean_reciprocal_rank\": mean_reciprocal_rank,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c90114c",
   "metadata": {},
   "source": [
    "## Apply the model on train and evaluate the desired metrics in leave-one-out crossval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b543e470",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T22:49:46.494798Z",
     "start_time": "2021-10-11T22:49:46.494762Z"
    }
   },
   "outputs": [],
   "source": [
    "# Takes 25 min\n",
    "\n",
    "nb_users_to_predict = 20\n",
    "train_predicted_user_ids_list = []\n",
    "\n",
    "# For each question in train\n",
    "for train_question_idx in tqdm(range(len(questions_train_array))):\n",
    "    train_question_id = question_index_to_question_id_train[train_question_idx]\n",
    "    user_scores = defaultdict(float)\n",
    "    # Get the 200 closest train questions\n",
    "    for (close_train_question_idx, weight) in zip(\n",
    "        labels[train_question_idx],\n",
    "        question_similarities[train_question_idx],\n",
    "    ):\n",
    "        # Remove the question itself\n",
    "        if close_train_question_idx == train_question_idx:\n",
    "            continue\n",
    "        # Get every close question id and respondents\n",
    "        close_train_question_id = question_index_to_question_id_train[close_train_question_idx]\n",
    "        for user_id in answered_question_id_to_user_ids[close_train_question_id]:\n",
    "            if np.isnan(user_id):\n",
    "                continue\n",
    "            # Add weight as a score for the respondent\n",
    "            user_scores[user_id] += weight\n",
    "    predicted_user_ids = []\n",
    "    # Keep 20 best users, while removing those who either subscribed too late, connected too long ago or asked\n",
    "    # the question\n",
    "    for user_id, weight in sorted(user_scores.items(), key=lambda x: -x[1]):\n",
    "        if len(predicted_user_ids) == nb_users_to_predict:\n",
    "            break\n",
    "        user_idx = user_id_to_idx[user_id]\n",
    "        # Check that user_id has possibly answered\n",
    "        # (i.e. is not the asker nor has subscribed too late nor connected too long ago)\n",
    "        if (\n",
    "            inscription_timestamps[user_idx]\n",
    "            <= asked_question_timestamps_train[train_question_idx]\n",
    "            <= last_connection_timestamps_plus_one_day[user_idx]\n",
    "        ) and user_id != asked_question_id_to_user_id[train_question_id]:\n",
    "            predicted_user_ids.append(user_id)\n",
    "    train_predicted_user_ids_list.append(predicted_user_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d7291036",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T23:55:29.017132Z",
     "start_time": "2021-10-11T23:55:12.275422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserting we have 20 predictions for each question and they are non null/nan...\n"
     ]
    }
   ],
   "source": [
    "print(\"Asserting we have 20 predictions for each question and they are non null/nan...\")\n",
    "\n",
    "for predictions in train_predicted_user_ids_list:\n",
    "    assert all([not np.isnan(p) and p is not None for p in predictions]) and len(predictions) == 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67982e31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T22:55:35.831273Z",
     "start_time": "2021-10-11T22:55:15.021900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carrying out leave-one-out kfold metrics estimation...\n",
      "ERROR! Session/line number was not unique in database. History logging moved to new session 123\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a199c8fbf61a4c4192f3082412f8ef80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing metrics scores...\n",
      "precision@1: 0.040229111210084205\n",
      "precision@5: 0.01950345427615565\n",
      "precision@20: 0.007589360622287591\n",
      "mean_reciprocal_rank: 0.06493295795350325\n"
     ]
    }
   ],
   "source": [
    "all_user_ids = list(user_id_to_idx.keys())\n",
    "results_over_batches = defaultdict(list)\n",
    "\n",
    "print(\"Carrying out leave-one-out kfold metrics estimation...\")\n",
    "for question_idx, ordered_user_ids in tqdm(enumerate(train_predicted_user_ids_list)):\n",
    "        results = get_metrics_on_ordered_user_ids(\n",
    "            ordered_user_ids,\n",
    "            answered_question_id_to_user_ids[question_index_to_question_id_train[question_idx]],\n",
    "            all_user_ids,\n",
    "        )\n",
    "        for metric, score in results.items():\n",
    "            results_over_batches[metric].append(score)\n",
    "\n",
    "print(\"Printing metrics scores...\")\n",
    "for metric, scores in results_over_batches.items():\n",
    "    print(f\"{metric}: {np.mean(scores)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f21946",
   "metadata": {},
   "source": [
    "In 4% of cases the first recommendation is actually correct.\n",
    "\n",
    "In 2*5 = 10% of cases there is a correct recommendation in the top 5 items.\n",
    "\n",
    "In 0.75*20 = 15% of cases there is a correct recommendation in the top 20 items.\n",
    "\n",
    "Mean reciprocal rank is not perfectly accurate because, in this algorithm, the top 20 users have been retrieved before computing the metrics. An exact score would mean sorting all users before computing metrics on those 5 million users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e701d64",
   "metadata": {},
   "source": [
    "## Dump predictions for test questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d2ce84",
   "metadata": {},
   "source": [
    "## Get metrics scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1709a775",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T23:13:57.281240Z",
     "start_time": "2021-10-11T23:06:32.715329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3279e822e5e14d9c8169aa085a5e2dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120742 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Same cell as before, but on test questions. Takes 5 min.\n",
    "\n",
    "nb_users_to_predict = 20\n",
    "test_predicted_user_ids_list = []\n",
    "\n",
    "for test_question_idx in tqdm(range(len(questions_test_array))):\n",
    "    test_question_id = question_index_to_question_id_test[test_question_idx]\n",
    "    user_scores = defaultdict(float)\n",
    "    for (close_train_question_idx, weight) in zip(\n",
    "        labels[len(questions_train_array) + test_question_idx],\n",
    "        question_similarities[len(questions_train_array) + test_question_idx],\n",
    "    ):\n",
    "        close_train_question_id = question_index_to_question_id_train[close_train_question_idx]\n",
    "        for user_id in answered_question_id_to_user_ids[close_train_question_id]:\n",
    "            if np.isnan(user_id):\n",
    "                continue\n",
    "            user_scores[user_id] += weight # * score if collaborative filtering\n",
    "    predicted_user_ids = []\n",
    "    for user_id, weight in sorted(user_scores.items(), key=lambda x: -x[1]):\n",
    "        if len(predicted_user_ids) == nb_users_to_predict:\n",
    "            break\n",
    "        user_idx = user_id_to_idx[user_id]\n",
    "        if (\n",
    "            inscription_timestamps[user_idx]\n",
    "            <= asked_question_timestamps_test[test_question_idx]\n",
    "            <= last_connection_timestamps_plus_one_day[user_idx]\n",
    "        ) and user_id != asked_question_id_to_user_id[test_question_id]:\n",
    "            predicted_user_ids.append(user_id)\n",
    "    test_predicted_user_ids_list.append(predicted_user_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "85663b96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T23:55:33.480610Z",
     "start_time": "2021-10-11T23:55:29.099971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assert we have 20 predictions for each question and they are non null/nan...\n"
     ]
    }
   ],
   "source": [
    "print(\"Assert we have 20 predictions for each question and they are non null/nan...\")\n",
    "\n",
    "for predictions in test_predicted_user_ids_list:\n",
    "    assert all([not np.isnan(p) and p is not None for p in predictions]) and len(predictions) == 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1f1acaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T23:14:04.705752Z",
     "start_time": "2021-10-11T23:14:00.749157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing predictions dataframe...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4362deb41104beebc93651288b9d5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>user_id_1</th>\n",
       "      <th>user_id_2</th>\n",
       "      <th>user_id_3</th>\n",
       "      <th>user_id_4</th>\n",
       "      <th>user_id_5</th>\n",
       "      <th>user_id_6</th>\n",
       "      <th>user_id_7</th>\n",
       "      <th>user_id_8</th>\n",
       "      <th>user_id_9</th>\n",
       "      <th>...</th>\n",
       "      <th>user_id_11</th>\n",
       "      <th>user_id_12</th>\n",
       "      <th>user_id_13</th>\n",
       "      <th>user_id_14</th>\n",
       "      <th>user_id_15</th>\n",
       "      <th>user_id_16</th>\n",
       "      <th>user_id_17</th>\n",
       "      <th>user_id_18</th>\n",
       "      <th>user_id_19</th>\n",
       "      <th>user_id_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60382284</td>\n",
       "      <td>11987538</td>\n",
       "      <td>10676716</td>\n",
       "      <td>3730754</td>\n",
       "      <td>3962914</td>\n",
       "      <td>12208910</td>\n",
       "      <td>8245406</td>\n",
       "      <td>6257435</td>\n",
       "      <td>4721734</td>\n",
       "      <td>2270762</td>\n",
       "      <td>...</td>\n",
       "      <td>11241027</td>\n",
       "      <td>4403732</td>\n",
       "      <td>1292050</td>\n",
       "      <td>1771254</td>\n",
       "      <td>3460670</td>\n",
       "      <td>1158845</td>\n",
       "      <td>12917563</td>\n",
       "      <td>1527780</td>\n",
       "      <td>7108653</td>\n",
       "      <td>9841389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60471556</td>\n",
       "      <td>1126841</td>\n",
       "      <td>8881863</td>\n",
       "      <td>65863</td>\n",
       "      <td>1491895</td>\n",
       "      <td>209103</td>\n",
       "      <td>3962914</td>\n",
       "      <td>12500315</td>\n",
       "      <td>882003</td>\n",
       "      <td>159145</td>\n",
       "      <td>...</td>\n",
       "      <td>2554330</td>\n",
       "      <td>404970</td>\n",
       "      <td>817643</td>\n",
       "      <td>3309790</td>\n",
       "      <td>5382650</td>\n",
       "      <td>60761</td>\n",
       "      <td>11781098</td>\n",
       "      <td>10771714</td>\n",
       "      <td>845414</td>\n",
       "      <td>2877241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60472988</td>\n",
       "      <td>1144035</td>\n",
       "      <td>10676716</td>\n",
       "      <td>974731</td>\n",
       "      <td>4473615</td>\n",
       "      <td>303298</td>\n",
       "      <td>7297700</td>\n",
       "      <td>4985099</td>\n",
       "      <td>330315</td>\n",
       "      <td>10419279</td>\n",
       "      <td>...</td>\n",
       "      <td>17389</td>\n",
       "      <td>8258942</td>\n",
       "      <td>1702057</td>\n",
       "      <td>90513</td>\n",
       "      <td>2847159</td>\n",
       "      <td>4604579</td>\n",
       "      <td>9216423</td>\n",
       "      <td>20107</td>\n",
       "      <td>2561714</td>\n",
       "      <td>8609411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60481026</td>\n",
       "      <td>8620333</td>\n",
       "      <td>5641669</td>\n",
       "      <td>7867822</td>\n",
       "      <td>7897395</td>\n",
       "      <td>2000051</td>\n",
       "      <td>10338175</td>\n",
       "      <td>616443</td>\n",
       "      <td>10289265</td>\n",
       "      <td>391715</td>\n",
       "      <td>...</td>\n",
       "      <td>6257435</td>\n",
       "      <td>6622587</td>\n",
       "      <td>463319</td>\n",
       "      <td>3564359</td>\n",
       "      <td>5899756</td>\n",
       "      <td>10682983</td>\n",
       "      <td>11307127</td>\n",
       "      <td>6766919</td>\n",
       "      <td>8342999</td>\n",
       "      <td>11608064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60486309</td>\n",
       "      <td>295783</td>\n",
       "      <td>1048572</td>\n",
       "      <td>157247</td>\n",
       "      <td>12364678</td>\n",
       "      <td>1447675</td>\n",
       "      <td>1823841</td>\n",
       "      <td>4976422</td>\n",
       "      <td>9515207</td>\n",
       "      <td>2576254</td>\n",
       "      <td>...</td>\n",
       "      <td>218196</td>\n",
       "      <td>4964569</td>\n",
       "      <td>14104</td>\n",
       "      <td>9253414</td>\n",
       "      <td>3214431</td>\n",
       "      <td>7237224</td>\n",
       "      <td>3606008</td>\n",
       "      <td>12128278</td>\n",
       "      <td>830905</td>\n",
       "      <td>8754782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id  user_id_1  user_id_2  user_id_3  user_id_4  user_id_5  \\\n",
       "0     60382284   11987538   10676716    3730754    3962914   12208910   \n",
       "1     60471556    1126841    8881863      65863    1491895     209103   \n",
       "2     60472988    1144035   10676716     974731    4473615     303298   \n",
       "3     60481026    8620333    5641669    7867822    7897395    2000051   \n",
       "4     60486309     295783    1048572     157247   12364678    1447675   \n",
       "\n",
       "   user_id_6  user_id_7  user_id_8  user_id_9  ...  user_id_11  user_id_12  \\\n",
       "0    8245406    6257435    4721734    2270762  ...    11241027     4403732   \n",
       "1    3962914   12500315     882003     159145  ...     2554330      404970   \n",
       "2    7297700    4985099     330315   10419279  ...       17389     8258942   \n",
       "3   10338175     616443   10289265     391715  ...     6257435     6622587   \n",
       "4    1823841    4976422    9515207    2576254  ...      218196     4964569   \n",
       "\n",
       "   user_id_13  user_id_14  user_id_15  user_id_16  user_id_17  user_id_18  \\\n",
       "0     1292050     1771254     3460670     1158845    12917563     1527780   \n",
       "1      817643     3309790     5382650       60761    11781098    10771714   \n",
       "2     1702057       90513     2847159     4604579     9216423       20107   \n",
       "3      463319     3564359     5899756    10682983    11307127     6766919   \n",
       "4       14104     9253414     3214431     7237224     3606008    12128278   \n",
       "\n",
       "   user_id_19  user_id_20  \n",
       "0     7108653     9841389  \n",
       "1      845414     2877241  \n",
       "2     2561714     8609411  \n",
       "3     8342999    11608064  \n",
       "4      830905     8754782  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Computing predictions dataframe...\")\n",
    "\n",
    "test_question_id_and_sorted_20_user_ids = [\n",
    "    [question_index_to_question_id_test[question_idx]] + prediction\n",
    "    for question_idx, prediction in tqdm(enumerate(test_predicted_user_ids_list))\n",
    "]\n",
    "\n",
    "test_question_id_and_sorted_20_user_ids_df = pd.DataFrame(\n",
    "    test_question_id_and_sorted_20_user_ids,\n",
    "    columns=[\"question_id\"] + [f\"user_id_{i}\" for i in range(1, 21)],\n",
    ").astype(\"int32\")\n",
    "\n",
    "test_question_id_and_sorted_20_user_ids_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f012791",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T23:17:46.844522Z",
     "start_time": "2021-10-11T23:17:44.518428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumping predictions to csv...\n"
     ]
    }
   ],
   "source": [
    "print(\"Dumping predictions to csv...\")\n",
    "\n",
    "test_question_id_and_sorted_20_user_ids_df.to_csv(\n",
    "    \"/Users/victormoeneclaey/pickles/question_id_and_sorted_20_user_ids.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f0f992",
   "metadata": {},
   "source": [
    "# Try a more \"elaborate\" model: ALS on train data with spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2657026b",
   "metadata": {},
   "source": [
    "We here want to try a more complicated model on the end of the funnel, i.e. given a train question, choose the users that are very probable to answer. Of course, we can use the fact that we know the users who have answered this question. But, studying the questions/users graph, we might find that other users who would have been able to answer as well, and give a score of likelihood.\n",
    "\n",
    "Collaborative filtering is meant to solve this problem. Some collaborative filtering algorithms like ALS (Alternating Least Squares) work by assigning each question and each user a latent representation (fixed-sized vector) and computing a score for every pair (question, user) as a simple scalar product. We try to use that technique to make the model more general and regular.\n",
    "\n",
    "We use spark for the size of the dataset is big, and there is a pre-built ALS algorithm with MLlib.\n",
    "\n",
    "Spark setup On MacOS: https://www.luminis.eu/blog/how-to-install-pyspark-and-apache-spark-on-macos/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe75b58",
   "metadata": {},
   "source": [
    "## Create the Spark cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29df5a6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T23:14:16.304613Z",
     "start_time": "2021-10-11T23:14:04.712400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Spark session using findspark lib...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/12 01:14:09 WARN Utils: Your hostname, MacBook-Pro-7.local resolves to a loopback address: 127.0.0.1; using 192.168.0.23 instead (on interface en0)\n",
      "21/10/12 01:14:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/10/12 01:14:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "print(\"Initializing Spark session using findspark lib...\")\n",
    "\n",
    "os.environ['SPARK_HOME'] = 'py_38_env/lib/python3.8/site-packages/pyspark'\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"15g\").master(\"local[*]\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9793872e",
   "metadata": {},
   "source": [
    "## Insert the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "614cba01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T23:58:28.778009Z",
     "start_time": "2021-10-11T23:57:57.339456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting answers data into the cluster...\n"
     ]
    }
   ],
   "source": [
    "print(\"Inserting answers data into the cluster...\")\n",
    "answers_train_spark = spark.createDataFrame(answers_train[{\"question_id\", \"user_id\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9212bdc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T00:10:54.439947Z",
     "start_time": "2021-10-12T00:10:54.089152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building matrix with the question-user interactions...\n",
      "For metrics computation, randomly splitting the data into fake train and fake test with a 80-20 ratio...\n",
      "root\n",
      " |-- question_id: long (nullable = true)\n",
      " |-- user_id: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "print(\"Building matrix with the question-user interactions...\")\n",
    "data = answers_train_spark.select([\"question_id\", \"user_id\"])\n",
    "\n",
    "def get_binary_data(data):\n",
    "    data = data.withColumn(\n",
    "        \"answered\", lit(1)\n",
    "    ).withColumn(\n",
    "        \"user_id\", col(\"user_id\").cast(\"int\")\n",
    "    ).withColumn(\n",
    "        \"question_id\", col(\"question_id\").cast(\"int\")\n",
    "    )\n",
    "    data = data.select([\"question_id\", \"user_id\", \"answered\"])\n",
    "    return data\n",
    "\n",
    "binary_data = get_binary_data(data)\n",
    "\n",
    "print(\"For metrics computation, randomly splitting the data into fake train and fake test with a 80-20 ratio...\")\n",
    "train, test = binary_data.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Print the schema\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f2cf2b",
   "metadata": {},
   "source": [
    "## Training the model and get the regression metrics on fake train and fake test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd78f24e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T00:29:11.773134Z",
     "start_time": "2021-10-12T00:21:18.416792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building a recommendation model using Alternating Least Squares method...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/12 02:21:19 WARN TaskSetManager: Stage 323 contains a task of very large size (2680 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/10/12 02:21:21 WARN TaskSetManager: Stage 324 contains a task of very large size (2680 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Building a recommendation model using Alternating Least Squares method...\")\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "model = ALS(\n",
    "    userCol=\"question_id\",\n",
    "    itemCol=\"user_id\",\n",
    "    ratingCol=\"answered\",\n",
    "    nonnegative=True,\n",
    "    coldStartStrategy=\"drop\"\n",
    ").setImplicitPrefs(True).setRank(30).setRegParam(0).fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3acb1cb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T00:29:11.995926Z",
     "start_time": "2021-10-12T00:29:11.790636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining training and cross-validation metrics and evaluator...\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining training and cross-validation metrics and evaluator...\")\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator, RankingEvaluator\n",
    "\n",
    "# I was not succesful with using classification or ranking metrics and so I stuck to the classic regression metrics\n",
    "evaluator=RegressionEvaluator(metricName=\"rmse\", labelCol=\"answered\", predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f146c01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T00:31:04.769515Z",
     "start_time": "2021-10-12T00:29:12.014773Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/12 02:29:14 WARN TaskSetManager: Stage 669 contains a task of very large size (2680 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New rmse on train :  0.9602660308063711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/12 02:30:20 WARN TaskSetManager: Stage 698 contains a task of very large size (2680 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 702:===================================================> (195 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New rmse on test:  0.9983714903991764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array\n",
    "\n",
    "# Get metrics score\n",
    "predictions = model.transform(train).withColumn(\n",
    "    \"prediction\", col(\"prediction\").cast(\"double\")\n",
    ").withColumn(\"answered\", col(\"answered\").cast(\"double\"))\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"New rmse on train : \", rmse)\n",
    "\n",
    "predictions = model.transform(test).withColumn(\n",
    "    \"prediction\", col(\"prediction\").cast(\"double\")\n",
    ").withColumn(\"answered\", col(\"answered\").cast(\"double\"))\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"New rmse on test: \", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037a0e14",
   "metadata": {},
   "source": [
    "We see that, even without regularization, the RMSE on train is very high and the model seems to underfit.\n",
    "\n",
    "But adding hidden dimension (rank) makes training even longer. But went on with this very model but one big improvement would be to accelerate this training step and work with a bigger computation capacity (more CPUs, more storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1921be",
   "metadata": {},
   "source": [
    "## Try to cross-validate the hyperparameters (rank, regularization) but took too much time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1c7027",
   "metadata": {},
   "source": [
    "This step unfortunately took to long for me to run. I was quite disappointed, but still keep this part for further calculations on bigger machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e23484ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T00:31:06.101390Z",
     "start_time": "2021-10-12T00:31:04.791336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defining the cross-validation grid-search parameters...\n",
      "Defining a cross-validator object...\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "\n",
    "model = ALS(\n",
    "    userCol=\"question_id\",\n",
    "    itemCol=\"user_id\",\n",
    "    ratingCol=\"answered\",\n",
    "    nonnegative=True,\n",
    "    coldStartStrategy=\"drop\"\n",
    ").setImplicitPrefs(True)\n",
    "\n",
    "print(\"defining the cross-validation grid-search parameters...\")\n",
    "\n",
    "param_grid = ParamGridBuilder().addGrid(\n",
    "    model.regParam, [0.1, 0.05, 0.01, 0.001]\n",
    ").addGrid(model.rank, [5, 10, 20, 30, 100]).build()\n",
    "\n",
    "print(\"Defining a cross-validator object...\")\n",
    "# Setting up CV and adding parameters. We will perform a 5 fold cross-validation\n",
    "cross_validation = CrossValidator(\n",
    "    estimator = model,\n",
    "    estimatorParamMaps = param_grid,\n",
    "    evaluator = evaluator,\n",
    "    numFolds=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f12e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very long, has to be computed on a high CPU machine\n",
    "\n",
    "print(\"Running cross-validation, and choosing the best set of parameters\")\n",
    "best_model = cross_validation.fit(binary_data).bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017b156f",
   "metadata": {},
   "source": [
    "## Setting best_model to our previous model, rank=30 with no regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b311e01e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T00:40:22.656569Z",
     "start_time": "2021-10-12T00:31:06.108546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building a recommendation model using Alternating Least Squares method...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/12 02:31:06 WARN TaskSetManager: Stage 704 contains a task of very large size (2680 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/10/12 02:31:06 WARN TaskSetManager: Stage 705 contains a task of very large size (2680 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Building a recommendation model using Alternating Least Squares method...\")\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "model = ALS(\n",
    "    userCol=\"question_id\",\n",
    "    itemCol=\"user_id\",\n",
    "    ratingCol=\"answered\",\n",
    "    nonnegative=True,\n",
    "    coldStartStrategy=\"drop\"\n",
    ").setImplicitPrefs(True).setRank(30).setRegParam(0).fit(binary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "241224e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T00:40:22.736500Z",
     "start_time": "2021-10-12T00:40:22.678302Z"
    }
   },
   "outputs": [],
   "source": [
    "best_model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ad6ce5",
   "metadata": {},
   "source": [
    "## Get best recommendations per train question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47053cba",
   "metadata": {},
   "source": [
    "We here encountered another intractability problem: using the method recommendForAllUsers for best_model took way too long. We decided to use another technique with the hnswlib library we used before. This technique is also long because it requires to retrieve all the questions and users latent representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a17610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 3h (???)\n",
    "\n",
    "question_features = model.userFactors.toPandas()\n",
    "question_features_array = question_features.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462d068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 3s (???)\n",
    "\n",
    "user_features = model.itemFactors.toPandas()\n",
    "user_features_array = user_features.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "80813db5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T00:47:48.923023Z",
     "start_time": "2021-10-12T00:47:05.075410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the index...\n",
      "Inserting 159005 users in the index...\n",
      "Retrieving 20 best users per answer along with their scalar product...\n"
     ]
    }
   ],
   "source": [
    "import hnswlib\n",
    "\n",
    "p_inner_product = hnswlib.Index(space = 'ip', dim = 30)\n",
    "\n",
    "print(\"Initializing the index...\")\n",
    "p_inner_product.init_index(max_elements = len(user_features_array), ef_construction = 300, M = 16)\n",
    "\n",
    "print(f\"Inserting {len(user_features_array) - 1} users into the index...\")\n",
    "p_inner_product.add_items(np.array([user_feature[1] for user_feature in user_features_array[1:]]))\n",
    "\n",
    "print(\"Retrieving 20 best users per answer along with their scalar product...\")\n",
    "labels, distances = p_inner_product.knn_query(\n",
    "    np.array([question_feature[1] for question_feature in question_features_array]),\n",
    "    k = 20\n",
    ")\n",
    "question_user_inner_products = 1 - distances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5f6fc4a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T00:48:39.648287Z",
     "start_time": "2021-10-12T00:47:58.202360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping train questions with their predictions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b028b4b770434a30adb6ead9abb6ebba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Mapping train questions with their predictions\")\n",
    "\n",
    "question_id_to_als_predictions = {}\n",
    "\n",
    "for question_feature, user_indexes, similarities in tqdm(\n",
    "    zip(\n",
    "        question_features_array,\n",
    "        labels,\n",
    "        question_user_inner_products,\n",
    "    )\n",
    "):\n",
    "    question_id = question_feature[0]\n",
    "    # Only keep questions in train, because some answers are neither in train nor in test\n",
    "    if question_id not in question_id_to_question_index_train:\n",
    "        continue\n",
    "    # Get user ids with user indexes from knn, adding 1 because first profile is \"dummy\" profile with id 0 (???)\n",
    "    user_ids = [user_features_array[int(user_idx) + 1][0] for user_idx in user_indexes]\n",
    "    question_id_to_als_predictions[question_id] = list(zip(user_ids, similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2581a527",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T00:48:39.678539Z",
     "start_time": "2021-10-12T00:48:39.663462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserting that all train questions have their predictions...\n"
     ]
    }
   ],
   "source": [
    "print(\"Asserting that all train questions have their predictions...\")\n",
    "\n",
    "assert len(question_id_to_als_predictions) == len(questions_train_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546d19f8",
   "metadata": {},
   "source": [
    "## Evaluate the ALS model alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740022b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same script as usual, takes 15 min\n",
    "\n",
    "nb_users_to_predict = 20\n",
    "als_train_predicted_user_ids_list = []\n",
    "\n",
    "for train_question_idx in tqdm(range(len(questions_train_array))):\n",
    "    train_question_id = question_index_to_question_id_train[train_question_idx]\n",
    "    user_scores = defaultdict(float)\n",
    "    for (close_train_question_idx, weight) in zip(\n",
    "        labels[train_question_idx],\n",
    "        question_similarities[train_question_idx],\n",
    "    ):\n",
    "        if close_train_question_idx == train_question_idx:\n",
    "            continue\n",
    "        close_train_question_id = question_index_to_question_id_train[close_train_question_idx]\n",
    "        for user_id, score in question_id_to_als_predictions[close_train_question_id]:\n",
    "            if np.isnan(user_id):\n",
    "                continue\n",
    "            user_scores[user_id] += weight * score\n",
    "    predicted_user_ids = []\n",
    "    for user_id, weight in sorted(user_scores.items(), key=lambda x: -x[1]):\n",
    "        if len(predicted_user_ids) == nb_users_to_predict:\n",
    "            break\n",
    "        user_idx = user_id_to_idx[user_id]\n",
    "        if (\n",
    "            inscription_timestamps[user_idx]\n",
    "            <= asked_question_timestamps_train[train_question_idx]\n",
    "            <= last_connection_timestamps_plus_one_day[user_idx]\n",
    "        ) and user_id != asked_question_id_to_user_id[train_question_id]:\n",
    "            predicted_user_ids.append(user_id)\n",
    "    als_train_predicted_user_ids_list.append(predicted_user_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7be28fe1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T00:51:53.934669Z",
     "start_time": "2021-10-12T00:51:24.597413Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carrying out leave-one-out kfold metrics estimation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a547140ff74c6b96f652b7923c5dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing metrics scores...\n",
      "precision@1: 0.0008472557407273141\n",
      "precision@5: 0.001034107740271578\n",
      "precision@20: 0.0007520689405160179\n",
      "mean_reciprocal_rank: 0.003029064806208294\n"
     ]
    }
   ],
   "source": [
    "results_over_batches = defaultdict(list)\n",
    "\n",
    "print(\"Carrying out leave-one-out kfold metrics estimation...\")\n",
    "for question_idx, ordered_user_ids in tqdm(enumerate(als_train_predicted_user_ids_list)):\n",
    "        results = get_metrics_on_ordered_user_ids(\n",
    "            ordered_user_ids,\n",
    "            answered_question_id_to_user_ids[question_index_to_question_id_train[question_idx]],\n",
    "            all_user_ids,\n",
    "        )\n",
    "        for metric, score in results.items():\n",
    "            results_over_batches[metric].append(score)\n",
    "\n",
    "print(\"Printing metrics scores...\")\n",
    "for metric, scores in results_over_batches.items():\n",
    "    print(f\"{metric}: {np.mean(scores)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b9d49",
   "metadata": {},
   "source": [
    "The results are much weaker, as the model seems to underfit the training set. We still wish to try to merge this with the previous model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28c446b",
   "metadata": {},
   "source": [
    "## Merge of the ALS model with the \"simple\" model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3e0149",
   "metadata": {},
   "source": [
    "We wish to merge the two approaches: for each train question that is close to the test question, we give a score of 1 to the users that actually have answered to that question, and 0.5 x renormalized ALS score, where the renormalized ALS score is the ALS score divided by the biggest ALS score among the users for this train question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16e1fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same script as usual, takes 33 min\n",
    "\n",
    "nb_users_to_predict = 20\n",
    "als_merged_train_predicted_user_ids_list = []\n",
    "als_algorithm_ratio = 0.5\n",
    "\n",
    "for train_question_idx in tqdm(range(len(questions_train_array))):\n",
    "    train_question_id = question_index_to_question_id_train[train_question_idx]\n",
    "    user_scores = defaultdict(float)\n",
    "    for (close_train_question_idx, weight) in zip(\n",
    "        labels[train_question_idx],\n",
    "        question_similarities[train_question_idx],\n",
    "    ):\n",
    "        if close_train_question_idx == train_question_idx:\n",
    "            continue\n",
    "        close_train_question_id = question_index_to_question_id_train[close_train_question_idx]\n",
    "        for user_id in answered_question_id_to_user_ids[close_train_question_id]:\n",
    "            if np.isnan(user_id):\n",
    "                continue\n",
    "            user_scores[user_id] += weight\n",
    "        for user_id, score in question_id_to_als_predictions[close_train_question_id]:\n",
    "            if np.isnan(user_id):\n",
    "                continue\n",
    "            assert score <= question_id_to_als_predictions[close_train_question_id][0][1]\n",
    "            user_scores[user_id] += weight * score * als_algorithm_ratio / (\n",
    "                10**-10 + question_id_to_als_predictions[close_train_question_id][0][1]\n",
    "            )\n",
    "    predicted_user_ids = []\n",
    "    for user_id, weight in sorted(user_scores.items(), key=lambda x: -x[1]):\n",
    "        if len(predicted_user_ids) == nb_users_to_predict:\n",
    "            break\n",
    "        user_idx = user_id_to_idx[user_id]\n",
    "        # Check that user_id has possibly answered\n",
    "        # (is not the asker nor has subscribed too late nor connected too long ago)\n",
    "        if (\n",
    "            inscription_timestamps[user_idx]\n",
    "            <= asked_question_timestamps_train[train_question_idx]\n",
    "            <= last_connection_timestamps_plus_one_day[user_idx]\n",
    "        ) and user_id != asked_question_id_to_user_id[train_question_id]:\n",
    "            predicted_user_ids.append(user_id)\n",
    "    als_merged_train_predicted_user_ids_list.append(predicted_user_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "98342fbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T00:59:41.525022Z",
     "start_time": "2021-10-12T00:59:12.681720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carrying out leave-one-out kfold metrics estimation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04df6377db449bbb5a4606284a727ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing metrics scores...\n",
      "precision@1: 0.0018353755165877758\n",
      "precision@5: 0.0011811863651900111\n",
      "precision@20: 0.0005237863424031819\n",
      "mean_reciprocal_rank: 0.003661054235073217\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "all_user_ids = np.array(list(user_id_to_idx.keys()))\n",
    "results_over_batches = defaultdict(list)\n",
    "\n",
    "print(\"Carrying out leave-one-out kfold metrics estimation...\")\n",
    "for question_idx, ordered_user_ids in tqdm(enumerate(als_merged_train_predicted_user_ids_list)):\n",
    "        results = get_metrics_on_ordered_user_ids(\n",
    "            ordered_user_ids,\n",
    "            answered_question_id_to_user_ids[question_index_to_question_id_train[question_idx]],\n",
    "            all_user_ids,\n",
    "        )\n",
    "        for metric, score in results.items():\n",
    "            results_over_batches[metric].append(score)\n",
    "\n",
    "print(\"Printing metrics scores...\")\n",
    "for metric, scores in results_over_batches.items():\n",
    "    print(f\"{metric}: {np.mean(scores)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca7777e",
   "metadata": {},
   "source": [
    "We actually notice that the merge does not improve the \"simple\" model alone. The simple model is kept and this model gave us the final predictions for this assignment. Further improvements have been listed and will be done when I have some time. Let's mention them again:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04098b1d",
   "metadata": {},
   "source": [
    "# Ideas of improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d5f772",
   "metadata": {},
   "source": [
    "- Run the ALS model on a bigger and more powerful machine to gridsearch params xith cross-validation and to be able to increase the size of the embedding (the rank)\n",
    "\n",
    "- Build linear (or nonlinear) mergers using user features like reputation, up_votes, down_votes, the \"About me\" user section to offer new user recommendations. An example would be to train a merger on several features of user u  regarding test question q like:\n",
    "    - log(1 + reputation of u)\n",
    "    - log(1 + upvotes of u)\n",
    "    - the averaged ALS score of closest train questions for user u\n",
    "    - the tf-idf similarity between users that have responded to closest train questions and user u, or between the question asker and user u. To embed users, we can use the \"About me\" section, or the titles/contents of questions that a user has responded to\n",
    "    - the tf-idf similarity between user u and the question\n",
    "    - use the number of train questions the user u has responded to in order to give an idea of the user's activity\n",
    "\n",
    "- Grid-search the numerous \"magic\" hyperparameters we left on the way"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_38_env",
   "language": "python",
   "name": "py_38_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
